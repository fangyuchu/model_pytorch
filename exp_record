确认的手段：
bn_momentum 0.1
extractor和cnn同lr
先训练10个epoch的完整网络，然后在10~80个epoch中每10个epoch训练1个epoch的掩码
采用sgd
shotrcut_mask改成mask的绝对值的mean，一定程度上有助于稳定网络
batchsize=128
grad_clip=1
剪第一层
weight_decay 5e-4

还在进行的

block_penalty,怎么避免震荡，同时又不能出现太极端的剪法
#todo：shortcut mask在结构确定之后是不是可以去掉
#todo:所有mask在结构确定之后就自由更新
考虑不要一开始剪这么多，慢慢剪
考虑持续更新掩码


不采用的
blp设为（0.9 or 0.1）-shortcut_mask;会导致把后面全剪完
adam收敛太慢，520个epoch都没收敛
加大batchsize，对结果没有改进
不剪第一层，之前实现有bug才导致第一层剪完网络crash，现已更正




he schedule baseline
    weight decay 1e-4
    0.9121,0.9251,0.9226,0.9313,0.9216,0.9202
    weight decay 5e-4
    0.9380,0.9307,0.9267,0.9358,0.9293,0.9382
sfp schedule baseline
    weight decay 5e-4
    0.9436,0.9410,0.9396,0.9399,0.9405



resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_warmup10_adam_1 和2 没收敛，第二档lr可以延长一点

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_warmup10_sgd_1/flop=67391114,accuracy=0.91170.tar

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_warmup10_sgd_2/flop=67510922,accuracy=0.88680.tar

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_heschedule_weightdecay1_blPenalty_target2_3/flop=67520138,accuracy=0.91660.tar 网络结构比较均衡，大部分shortcut_mask在0.2上下

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_heschedule_weightdecay1_blPenalty_target5_2/flop=67474058,accuracy=0.92050.tar 网络比较均衡，大部分shortcut_mask在0.5上下

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_warmup10_sgd_blp0.5_1/flop=67363466,accuracy=0.92770.tar 网络结构比较均衡，训练掩码中的震荡也比较少;和baseline的loss比，感觉是过拟合了

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_warmup10_sgd_blp0.5_2/flop=67492490,accuracy=0.89370.tar 网络结构只保留了前面的层;loss看起来也比上面大一点点

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_warmup10_sgd_blpDynamic_1/flop=67529354,accuracy=0.91160.tar blp设为（0.9 or 0.1）-shortcut_mask;网络结构只保留了前面的层;

resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_mask80_warmup10_sgd_blpDynamic_2/flop=67483274,accuracy=0.90720.tar 网络结构基本只保留了前面的层;

#todo:确认bn_momentum 0.1；extractor和cnn同lr；先训练10个epoch的完整网络，然后在10~80个epoch中每10个epoch训练1个epoch的掩码
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_0.5_1/flop=67418762,accuracy=0.91620.tar
[4, 16, 16, 9, 9, 9, 10, 7, 9, 3, 10, 8, 9, 2, 10, 4, 13, 5, 13, 12, 20, 14, 19, 8, 19, 12, 21, 11, 20, 10, 23, 17, 21, 12, 23, 14, 25, 39, 46, 21, 34, 25, 37, 25, 37, 20, 32, 24, 33, 20, 28, 35, 35, 22, 25]
loss和val acc的曲线和baseline相似；但loss下降的没有baseline多，似乎还没收敛？
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_0.5_2/flop=67483274,accuracy=0.90870.tar
[7, 6, 7, 7, 9, 6, 9, 9, 12, 3, 8, 7, 11, 5, 10, 8, 8, 8, 8, 16, 14, 15, 13, 0, 0, 15, 32, 32, 32, 32, 32, 31, 29, 32, 32, 3, 15, 0, 0, 8, 14, 1, 3, 8, 14, 25, 17, 38, 34, 40, 52, 53, 56, 58, 37]

adam组
resnet56_predicted_mask_shortcut_with_weight_noFirstConv_wd5_adam_blpenalty0.5_128bs_absShortcutMask_1/flop=67437194,accuracy=0.84720.tar
[16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 26, 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
lr decay epoch [220, 360]; num epoch 520;还是没有收敛，adam速度太慢了，放弃了
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_adam_blpenalty0.5_2/flop=67501706,accuracy=0.86310.tar
[3, 3, 4, 4, 6, 2, 6, 4, 5, 2, 5, 4, 6, 6, 10, 7, 6, 4, 6, 3, 9, 5, 12, 9, 15, 8, 13, 6, 19, 10, 15, 3, 12, 12, 12, 8, 18, 7, 64, 46, 61, 63, 64, 64, 20, 64, 64, 64, 64, 64, 64, 64, 64, 63, 64]
同上

batchsize=512组
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_512bs_1/flop=67492490,accuracy=0.92050.tar
[6, 6, 9, 9, 8, 5, 7, 4, 7, 6, 7, 4, 9, 7, 10, 5, 9, 9, 12, 23, 27, 16, 24, 20, 16, 18, 15, 18, 13, 16, 15, 13, 17, 18, 14, 12, 15, 52, 36, 33, 25, 36, 29, 44, 35, 45, 34, 36, 43, 37, 25, 29, 36, 43, 36]
loss降在0.01以内了
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_512bs_2/flop=67464842,accuracy=0.91840.tar
[11, 6, 8, 9, 5, 6, 6, 5, 10, 9, 7, 9, 8, 12, 8, 7, 7, 11, 9, 28, 16, 18, 14, 17, 15, 9, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 53, 52, 64, 54, 60, 54, 63, 54, 53, 55, 59, 52, 58, 60, 56, 59, 58]

abs shortcut mask组
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_128bs_absShortcutMask_1/flop=67464842,accuracy=0.92090.tar
[5, 6, 6, 4, 8, 8, 10, 5, 9, 8, 12, 8, 10, 7, 8, 7, 10, 5, 10, 17, 18, 13, 14, 17, 25, 15, 20, 5, 16, 12, 17, 9, 19, 9, 16, 9, 18, 38, 41, 31, 29, 28, 33, 23, 36, 19, 30, 33, 50, 51, 43, 52, 49, 59, 46]
loss没有降到0.01以内，mask能相对保证的比以前稳定一点
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_128bs_absShortcutMask_2/flop=67492490,accuracy=0.91520.tar
[8, 7, 7, 5, 10, 5, 9, 6, 3, 9, 8, 7, 8, 10, 9, 7, 10, 4, 13, 17, 18, 15, 19, 17, 21, 11, 23, 7, 18, 6, 20, 6, 20, 3, 22, 6, 14, 35, 42, 30, 49, 17, 37, 24, 37, 30, 33, 15, 35, 42, 57, 61, 58, 63, 51]
同上

不剪第一层组
resnet56_predicted_mask_shortcut_with_weight_noFirstConv_wd5_sgd_blpenalty0.5_128bs_1/flop=67492490,accuracy=0.91310.tar
[16, 0, 0, 1, 6, 9, 10, 8, 11, 7, 11, 8, 12, 10, 14, 7, 14, 9, 12, 21, 25, 21, 29, 21, 28, 21, 29, 19, 27, 20, 26, 17, 22, 7, 15, 6, 17, 19, 17, 17, 17, 15, 10, 10, 7, 11, 6, 15, 13, 7, 22, 64, 64, 64, 64]
loss没降到0.01以内
resnet56_predicted_mask_shortcut_with_weight_noFirstConv_wd5_sgd_blpenalty0.5_128bs_2/flop=67474058,accuracy=0.91690.tar
[16, 10, 11, 10, 11, 5, 9, 6, 10, 7, 9, 7, 6, 8, 8, 1, 4, 1, 4, 2, 11, 6, 13, 11, 15, 11, 19, 9, 19, 14, 22, 15, 24, 19, 26, 19, 28, 32, 42, 42, 44, 40, 46, 40, 50, 36, 50, 37, 46, 26, 39, 25, 40, 29, 29]

以上的实验都存在如下的bug，最明显的问题就是mask全0时，该层就输出0，所以第一层剪掉网络就crash了
out = out +self.downsample(input)
out =out * self.shortcut_mask


#todo:确认;bn_momentum 0.1;同lr=0.1;在10~80个epoch中每10个epoch训练1个epoch的掩码;采用sgd;shotrcut_mask=mask的绝对值的mean;batchsize=128;grad_clip=1
no block penalty组
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenaltyNo_5/flop=67510922,accuracy=0.92560.tar
[8, 10, 8, 14, 9, 9, 11, 10, 9, 3, 4, 10, 7, 0, 0, 4, 6, 0, 0, 22, 8, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 7, 11, 14, 17, 17, 27, 58, 49, 35, 60, 53, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
网络结构在训练时相对比较稳定、变化较小，loss曲线在初期很平滑；val acc在0.01阶段有比较明显的下滑（类似过拟合）；loss最终在0.02上下;后面几层的shortcut_mask接近1；
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenaltyNo_6/flop=67464842,accuracy=0.90540.tar
[7, 4, 8, 4, 8, 6, 13, 9, 13, 5, 11, 6, 13, 3, 11, 4, 12, 2, 8, 11, 14, 12, 19, 13, 16, 7, 12, 11, 15, 7, 17, 8, 14, 7, 18, 10, 20, 35, 40, 45, 52, 37, 50, 35, 52, 26, 58, 29, 54, 33, 45, 39, 49, 32, 51]
网络还是有一定震荡，loss在初期也是大幅震荡，loss在0.04以上，是不是继续训练会有更好的结果？？？
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenaltyNo_7/flop=67501706,accuracy=0.91450.tar
[7, 6, 9, 9, 12, 10, 14, 10, 14, 6, 15, 7, 14, 7, 14, 8, 15, 12, 15, 16, 19, 22, 18, 15, 25, 16, 18, 11, 25, 20, 19, 11, 23, 6, 19, 8, 13, 27, 23, 15, 18, 15, 12, 10, 7, 5, 5, 2, 17, 14, 25, 14, 53, 64, 64]
掩码训练后期网络较稳定，前期大幅震荡；loss在0.04以上

block penalty 0.5组
4组训练mask时，总loss均在0.5左右
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_6/flop=67446410,accuracy=0.90470.tar
[7, 8, 6, 2, 5, 2, 4, 5, 6, 6, 5, 7, 3, 5, 6, 2, 3, 5, 2, 26, 25, 27, 20, 27, 14, 22, 13, 25, 15, 24, 15, 29, 18, 25, 12, 28, 17, 64, 43, 49, 35, 52, 32, 48, 27, 44, 26, 44, 33, 52, 25, 53, 29, 39, 26]
block_loss在0.05左右，大概是总loss的1/10；loss在0.04以上
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_7/flop=67529354,accuracy=0.92020.tar
[16, 15, 15, 15, 15, 13, 15, 12, 16, 7, 15, 4, 9, 9, 6, 8, 8, 4, 16, 27, 10, 7, 9, 13, 11, 7, 10, 5, 10, 8, 11, 0, 0, 15, 18, 12, 24, 55, 43, 40, 43, 28, 36, 23, 37, 24, 29, 10, 22, 9, 40, 42, 42, 44, 36]
block_loss在0.09左右，大概是总loss的1/5；loss在0.03以上
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_8/flop=67354250,accuracy=0.91360.tar
[7, 6, 11, 6, 9, 9, 11, 7, 9, 8, 11, 7, 12, 8, 9, 1, 10, 3, 9, 12, 18, 16, 15, 8, 19, 12, 14, 14, 22, 9, 19, 11, 17, 20, 22, 15, 14, 37, 36, 16, 34, 28, 42, 28, 37, 30, 53, 25, 52, 26, 48, 31, 51, 34, 37]
block_loss在0.04左右；loss大部分在0.04以上
resnet56_predicted_mask_shortcut_with_weight_pruneFirstConv_wd5_sgd_blpenalty0.5_9/flop=67538570,accuracy=0.91460.tar
[3, 4, 5, 6, 8, 9, 13, 5, 12, 8, 11, 7, 15, 8, 12, 6, 12, 3, 13, 13, 32, 15, 19, 11, 21, 6, 19, 11, 17, 9, 21, 8, 22, 12, 21, 6, 21, 33, 34, 19, 32, 28, 47, 16, 36, 25, 43, 26, 48, 29, 48, 28, 44, 26, 32]
block_loss在0.044左右；loss大部分在0.04以上


#todo:确认;bn_momentum 0.1;同lr=0.1;在10~80个epoch中每10个epoch训练1个epoch的掩码;采用sgd;shotrcut_mask=mask的绝对值的mean;batchsize=128;grad_clip=1
剪第一层;weight_decay 5e-4
训练时，上一次的掩码作为下一次掩码的target，把两个mask差的l1范数作为loss
model_saved/resnet56_predicted_mask_shortcut_with_weight_blpenaltyMaskDifNorm_1/flop=67510922,accuracy=0.91540.tar
[6, 8, 11, 1, 10, 9, 11, 8, 10, 7, 12, 6, 12, 5, 13, 10, 15, 6, 16, 24, 17, 14, 19, 10, 13, 6, 12, 6, 10, 7, 12, 10, 13, 16, 20, 20, 26, 43, 35, 23, 11, 18, 17, 18, 17, 20, 18, 27, 34, 33, 43, 64, 64, 64, 64]
每次开始和结束训练掩码时，掩码震荡不大，但每次开始训练掩码时，和上一次相比有很大震荡
model_saved/resnet56_predicted_mask_shortcut_with_weight_blpenaltyMaskDifNorm_2/flop=67372682,accuracy=0.92160.tar
[7, 10, 8, 12, 12, 13, 15, 13, 16, 10, 14, 11, 16, 6, 13, 8, 15, 10, 15, 26, 18, 14, 7, 12, 12, 8, 8, 7, 9, 15, 16, 9, 11, 11, 7, 14, 17, 60, 60, 46, 60, 43, 46, 47, 43, 19, 4, 0, 0, 0, 0, 0, 11, 25, 35]
同上
model_saved/resnet56_predicted_mask_shortcut_with_weight_blpenaltyMaskDifNorm_3/flop=67501706,accuracy=0.91060.tar
[5, 3, 6, 5, 9, 6, 11, 7, 11, 8, 14, 8, 14, 6, 15, 7, 15, 7, 15, 18, 27, 14, 28, 15, 29, 18, 31, 16, 26, 4, 3, 3, 6, 1, 1, 3, 1, 24, 37, 18, 39, 18, 41, 22, 30, 29, 34, 27, 35, 37, 48, 38, 55, 43, 57]
同上

#todo:确认;bn_momentum 0.1;同lr=0.1;采用sgd;shotrcut_mask=mask的绝对值的mean;batchsize=128;grad_clip=1;剪第一层;weight_decay 5e-4
更新掩码时使用上一次和这一次的掩码差的l1范数作为loss，训练掩码是连续训练20个epoch
model_saved/resnet56_predicted_mask_shortcut_with_weight_blpenaltyMaskDifNorm_continueUpdateMask_1/flop=67557002,accuracy=0.90460.tar
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 0, 7, 18, 21, 24, 18, 28, 21, 29, 23, 29, 22, 28, 18, 27, 15, 20, 39, 15, 62, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
网络结构不好
model_saved/resnet56_predicted_mask_shortcut_with_weight_blpenaltyMaskDifNorm_continueUpdateMask_2/flop=67464842,accuracy=0.91170.tar
[7, 10, 11, 13, 11, 10, 11, 9, 10, 6, 7, 5, 8, 9, 5, 1, 3, 9, 4, 22, 8, 3, 3, 1, 3, 12, 5, 17, 14, 25, 10, 26, 15, 27, 25, 28, 30, 63, 62, 50, 47, 40, 37, 40, 36, 28, 26, 24, 28, 30, 27, 40, 27, 47, 41]
网络结构相对稳定，但在后期有不小的变化
model_saved/resnet56_predicted_mask_shortcut_with_weight_blpenaltyMaskDifNorm_continueUpdateMask_3/flop=67354250,accuracy=0.91600.tar
[3, 11, 14, 11, 13, 14, 16, 10, 15, 12, 14, 12, 14, 7, 8, 11, 5, 11, 4, 20, 7, 15, 11, 11, 0, 13, 7, 23, 13, 16, 5, 15, 9, 24, 10, 26, 23, 63, 54, 41, 27, 30, 16, 34, 20, 17, 10, 18, 11, 30, 13, 20, 17, 52, 30]
网络结构相对稳定，有一些变化


#重新开始,改为shortcutmask无权重
resnet56_predicted_mask_and_shortcut_net_prune70_1/flop=35973770,accuracy=0.84600.tar
[4, 11, 10, 9, 8, 11, 5, 12, 5, 12, 5, 8, 4, 10, 9, 7, 8, 11, 9, 21, 11, 13, 6, 13, 8, 10, 3, 4, 2, 10, 5, 3, 2, 9, 1, 8, 5, 23, 2, 6, 4, 2, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 1, 0]
resnet56_predicted_mask_and_shortcut_net_prune70_2/flop=35872394,accuracy=0.88500.tar
[3, 8, 9, 7, 9, 9, 8, 5, 9, 7, 6, 8, 7, 7, 9, 5, 6, 5, 8, 17, 17, 13, 9, 19, 11, 15, 10, 5, 8, 6, 7, 3, 6, 5, 5, 7, 5, 15, 11, 2, 1, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 6, 2, 8, 10]
resnet56_predicted_mask_and_shortcut_net_prune70_3/flop=36056714,accuracy=0.88260.tar
[13, 11, 5, 8, 6, 10, 7, 6, 4, 8, 1, 2, 0, 2, 2, 2, 2, 2, 3, 2, 1, 1, 2, 2, 3, 3, 3, 1, 2, 2, 0, 3, 2, 3, 3, 4, 3, 14, 11, 4, 10, 13, 6, 5, 16, 13, 12, 22, 25, 39, 58, 64, 64, 64, 64]
resnet56_predicted_mask_and_shortcut_net_prune70_4/flop=36130442,accuracy=0.88620.tar
[15, 13, 9, 12, 8, 8, 6, 8, 6, 7, 9, 8, 5, 10, 6, 9, 4, 10, 5, 13, 15, 21, 9, 16, 9, 2, 2, 7, 7, 4, 2, 0, 6, 3, 1, 1, 4, 19, 5, 5, 5, 6, 3, 1, 0, 4, 1, 7, 2, 7, 6, 13, 4, 15, 3]


resnet56_predicted_mask_and_shortcut_net_prune80_1/flop=24969866,accuracy=0.88340.tar
[6, 14, 8, 8, 6, 8, 3, 4, 3, 4, 3, 3, 3, 4, 5, 4, 3, 3, 1, 10, 7, 4, 1, 2, 3, 1, 1, 1, 1, 2, 2, 2, 0, 3, 2, 4, 3, 27, 11, 5, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 37, 48, 47]
resnet56_predicted_mask_and_shortcut_net_prune80_2/flop=25015946,accuracy=0.85120.tar
[3, 2, 2, 2, 1, 1, 3, 1, 3, 0, 0, 2, 3, 1, 2, 2, 0, 0, 1, 3, 9, 4, 5, 7, 3, 9, 2, 8, 7, 9, 6, 15, 3, 12, 4, 9, 3, 22, 21, 24, 8, 23, 7, 21, 5, 22, 12, 24, 6, 17, 4, 19, 9, 23, 14]


#todo:
extractor中inner feature采用对conv.weight进行pca。之后和cross-layer feature一起喂入全连接产生掩码。
先训练extractor+cnn共20个epoch，产生掩码（期间不进行置0），训练完了后置0并prune。剪的过程中某层剪的太厉害了（0.1）才加shortcut。
shortcut中输入输出feature map大小不一样则采用1x1conv，feature map数量不一样就直接补0.
95%
resnet56_predicted_mask_and_variable_shortcut_net_newinner_95_doubleschedule_1/flop=6389538,accuracy=0.86560.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_95_doubleschedule_2/flop=6519378,accuracy=0.86530.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_95_doubleschedule_3/flop=6537524,accuracy=0.79830.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_95_4/flop=6514804,accuracy=0.86140.tar
90%
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_90_1/flop=12831484,accuracy=0.88630.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_90_2/flop=12717676,accuracy=0.88770.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_90_3/flop=12668322,accuracy=0.81590.tar 第一层被剪光
resnet56_predicted_mask_and_variable_shortcut_net_newinner_90_4/flop=12822882,accuracy=0.88650.tar
85%
resnet56_predicted_mask_and_variable_shortcut_net_newinner_85_1/flop=19168174,accuracy=0.90230.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_85_2/flop=19039636,accuracy=0.89520.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_85_3/flop=18993256,accuracy=0.89550.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_85_4/flop=19082408,accuracy=0.90120.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_85_doubleschedule_1/flop=19168174,accuracy=0.89930.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_85_doubleschedule_2/flop=19039636,accuracy=0.90140.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_85_doubleschedule_3/flop=18993256,accuracy=0.89430.tar
80%
resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_1/flop=25224460,accuracy=0.90540.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_2/flop=25248814,accuracy=0.90930.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_3/flop=25058616,accuracy=0.90350.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_4/flop=25303922,accuracy=0.90490.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_doubleschedule_1/flop=25224460,accuracy=0.91010.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_doubleschedule_2/flop=25248814,accuracy=0.91170.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_doubleschedule_3/flop=25058616,accuracy=0.90860.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_80_5schedule_1/flop=25224460,accuracy=0.90690.tar
75%
1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_75_1/flop=31270550,accuracy=0.90590.tar
1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_75_2/flop=31382466,accuracy=0.90970.tar
1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_75_4/flop=31431622,accuracy=0.91400.tar
70%
resnet56_predicted_mask_and_variable_shortcut_net_newinner_70_1/flop=37743924,accuracy=0.91160.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_70_2/flop=37404940,accuracy=0.91440.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_70_4/flop=37581432,accuracy=0.91370.tar

50%
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_50_doubleschedule_1/flop=62918400,accuracy=0.92370.tar
/1070ti/resnet56_predicted_mask_and_variable_shortcut_net_newinner_50_doubleschedule_2/flop=62920462,accuracy=0.92320.tar
resnet56_predicted_mask_and_variable_shortcut_net_newinner_50_doubleschedule_wd12/flop=62920462,accuracy=0.92460.tar

vgg16bn_cifar10
98%
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_98_2/flop=5742194,accuracy=0.86990.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_98_1/flop=5703534,accuracy=0.84920.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_98_3/flop=5729570,accuracy=0.81400.tar
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_98_5/flop=5762286,accuracy=0.85930.tar
95%
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_95_1/flop=15142462,accuracy=0.89130.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_95_2/flop=15219230,accuracy=0.89680.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_doubleschedule95_2/flop=15219230,accuracy=0.89820.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_95_3/flop=15222326,accuracy=0.86720.tar
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_95_5/flop=15138810,accuracy=0.89430.tar
90%
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_90_1/flop=30968242,accuracy=0.90780.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_90_2/flop=30998122,accuracy=0.90630.tar
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_90_3/flop=30823106,accuracy=0.91690.tar
87%
/1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_87_1/checkpoint/flop=41797042,accuracy=0.91860
/1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_87_2/checkpoint/flop=41796414,accuracy=0.90830
/1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_87_5/checkpoint/flop=41805974,accuracy=0.91090
85%
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_85_1/flop=46641614,accuracy=0.92250.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_85_2/flop=46663566,accuracy=0.91330.tar
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_85_3/flop=46725462,accuracy=0.91680.tar
80%
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_80_1/flop=62484162,accuracy=0.92700.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_80_2/flop=62338234,accuracy=0.92280.tar
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_80_3/flop=62450138,accuracy=0.91900.tar
75%
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_75_1/flop=79330634,accuracy=0.92950.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_75_2/flop=79478986,accuracy=0.92650.tar
vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_75_5/flop=79544546,accuracy=0.92690.tar
70%
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_70_1/flop=95002034,accuracy=0.93140.tar
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_70_2/flop=95303010,accuracy=0.92530.tar
1070ti/vgg16bn_predicted_mask_and_variable_shortcut_net_newinner_70_5/flop=95291990,accuracy=0.92590.tar

vgg16bn_cifar100
70%
1070ti/vgg16bn_cifar100_predicted_mask_and_variable_shortcut_net_newinner_70_5/flop=95399452,accuracy=0.69050.tar
75%
1070ti/vgg16bn_cifar100_predicted_mask_and_variable_shortcut_net_newinner_75_5/flop=79365860,accuracy=0.68310.tar
80%
1070ti/vgg16bn_cifar100_predicted_mask_and_variable_shortcut_net_newinner_80_5/flop=63208124,accuracy=0.67560.tar
85%
1070ti/vgg16bn_cifar100_predicted_mask_and_variable_shortcut_net_newinner_85_5/flop=47521548,accuracy=0.67430.tar
90%
1070ti/vgg16bn_cifar100_predicted_mask_and_variable_shortcut_net_newinner_90_5/flop=31438292,accuracy=0.66300.tar
95%
1070ti/vgg16bn_cifar100_predicted_mask_and_variable_shortcut_net_newinner_95_5/flop=15243784,accuracy=0.62400.tar

resnet50
80%
resnet50_predicted_mask_and_variable_shortcut_net_newinner_80_1/flop=826629664,accuracy=0.66424.tar
/home/victorfang/model_pytorch/data/model_saved/resnet50_predicted_mask_and_variable_shortcut_net_newinner_80_2/checkpoint/flop=826009567,accuracy=0.66506.tar
90%
/home/victorfang/model_pytorch/data/model_saved/resnet50_predicted_mask_and_variable_shortcut_net_newinner_90_4/checkpoint/flop=413158280,accuracy=0.47228.tar